---
layout: project
urltitle:  "Visually Grounded Interaction and Language (ViGIL)"
title: "Visually Grounded Interaction and Language (ViGIL)"
categories: nips, neurips, vancouver, canada, workshop, computer vision, natural language, grounding, interaction, machine learning, vigil, 2019, neurips19
permalink: /
bibtex: true
paper: true
acknowledgements: ""
---

<br>
<div class="row">
  <div class="col-xs-12">
    <center><h1>Visually Grounded Interaction and Language (ViGIL)</h1></center>
    <center><h2>NeurIPS 2019 Workshop, Vancouver, Canada</h2></center>
    <center><span style="color:#e74c3c;font-weight:400;"></span></center>
  </div>
</div>

<hr>

<!-- <div class="row" id="intro"> -->
  <!-- <div class="col-md-12"> -->
    <!-- <img src="{{ "/static/img/splash.png" | prepend:site.baseurl }}"> -->
    <!-- <p> Image credit: [2, 28, 12, 11, 15-21, 26]</p> -->
  <!-- </div> -->
<!-- </div> -->

<br>
<div class="row" id="intro">
  <div class="col-xs-12">
    <h2>Introduction</h2>
  </div>
</div>
<div class="row">
  <div class="col-xs-12">
    <p>
    The dominant paradigm in modern natural language understanding is learning statistical language models from text-only corpora. This approach is founded on a distributional notion of semantics, i.e. that the "meaning" of a word is based only on its relationship to other words. While effective for many applications, methods in this family suffer from limited semantic understanding, as they miss learning from the multimodal and interactive environment in which communication often takes place - the symbols of language thus are not grounded in anything concrete. The symbol grounding problem first highlighted this limitation, that "meaningless symbols (i.e.) words cannot be grounded in anything but other meaningless symbols" [18].
</p>

<p>
On the other hand, humans acquire language by communicating about and interacting within a rich, perceptual environment. This behavior provides the necessary grounding for symbols, i.e. to concrete objects or concepts (i.e. physical or psychological). Thus, recent work has aimed to bridge vision, interactive learning, and natural language understanding through language learning tasks based on natural images (ReferIt [1], GuessWhat?! [2], Visual Question Answering [3,4,5,6], Visual Dialog [7,8], Captioning [9], Visual-Audio Correspondence [30]) or through embodied agents performing interactive tasks [14,15,18,19,24,25,26,27,29] in physically simulated environments (DeepMind Lab [10], Baidu XWorld [11], OpenAI Universe [12], House3D [22], Matterport3D [23], GIBSON [27], MINOS [28], AI2-THOR [21], StreetLearn [18]), often drawing on the recent successes of deep learning and reinforcement learning. We believe this line of research poses a promising, long-term solution to the grounding problem faced by current, popular language understanding models.
</p>


<p>
While machine learning research exploring visually-grounded language learning may be in its earlier stages, it may be possible to draw insights from the rich research literature on human language acquisition. In neuroscience, recent progress in fMRI technology has enabled to better understand the interleave between language, vision and other modalities [15,16] suggesting that the brains shares neural representation of concepts across vision and language. Differently, developmental cognitive scientists have also argued that children acquiring various words is closely linked to them learning the underlying concept in the real world [12].
</p>

<p>
This workshop thus aims to gather people from various backgrounds - machine learning, computer vision, natural language processing, neuroscience, cognitive science, psychology, and philosophy - to share and debate their perspectives on why grounding may (or may not) be important in building machines that truly understand natural language.

</p>
  </div>
</div> <br>

<div class="row" id="dates">
  <div class="col-xs-12">
    <h2>Important Dates</h2>
  </div>
</div>

<div class="row">
  <div class="col-xs-12">
    <table class="table table-striped">
      <tbody>
        <tr>
          <td>Paper Submission Deadline</td>
          <td>September 9, 2019 - Midnight Pacific Time</td>
        </tr>
        <tr>
          <td>Final Decisions</td>
          <td>September 16, 2019 </td>
        </tr>
        <tr>
          <td>Workshop Date</td>
          <td>December 13 or 14, 2019</td>
        </tr>
      </tbody>
    </table>
  </div>
</div><br>

<div class="row" id="cfp">
  <div class="col-xs-12">
    <h2>Call for Papers</h2>
  </div>
</div>
<div class="row">
  <div class="col-xs-12">
    <p>
      We invite high-quality paper submissions on the following topics:
    </p>

  <p>
  <ul>
  <li>language acquisition or learning through interactions</li>
  <li>visual captioning, dialog, and question-answering</li>
  <li>reasoning in language and vision</li>
  <li>visual synthesis from language</li>
  <li>transfer learning in language and vision tasks</li>
  <li>navigation in virtual worlds with natural-language instructions</li>
  <li>machine translation with visual cues</li>
  <li>novel tasks that combine language, vision and actions</li>
  <li>understanding and modeling the relationship between language and vision in humans</li>
  <li>semantic systems and modeling of natural language and visual stimuli representations in the human brain</li>
  <li> audio visual scene-aware dialog systems - Visual Question Answering</li>
  <li> image/video captioning</li>
  <li> lip reading</li>
  <li> audio-visual scene understanding - Sound localization </li>
  <li> audio-visual speech processing </li>
  <li> audio-visual fusion </li>
</ul>
    </p>


  <p> <span style="font-weight:500;">Submission:</span>
  <br/>
    Accepted papers
    will be presented during joint poster sessions, with exceptional submissions
    selected for spotlight oral presentation. Accepted papers will be made publicly
    available as non-archival reports,
    allowing future submissions to archival conferences or journals.
  </p>

  <p>
  Submissions should be up to 4 pages excluding references, acknowledgements, and supplementary material, and should be
      <span style="color:#1a1aff;font-weight:400;"><a href="https://neurips.cc/Conferences/2019/PaperInformation/StyleFiles">NeurIPS format</a></span> and anonymous. The review process is double-blind.
  </p>
  <p>
  We also welcome published papers that are within the scope of the workshop (without re-formatting). This specific papers do not have to be anonymous. They are eligible for poster sessions and will only have a very light review process.
  </p>

  <!-- <p>
  Please submit your paper to the following address: <a href="https://cmt3.research.microsoft.com/VIGIL2018">https://cmt3.research.microsoft.com/VIGIL2018</a>
  If you have any question, send an email to: vigilworkshop2018@gmail.com
  </p> -->

  <!-- <p><b>Accepted workshop papers are eligible to the pool of reserved conference tickets (one ticket per accepted paper).</b>
  </p> -->

  </div>

</div><br>




<div class="row" id="schedule">
  <div class="col-xs-12">
    <h2>Schedule</h2>
  </div>
</div>

TBA

<br>
<div class="row" id="speakers">
  <div class="col-xs-12">
    <h2>Invited Speakers</h2>
  </div>
</div>

TBA

<br>
<div class="row" id="organizers">
  <div class="col-xs-12">
    <h2>Organizers</h2>
  </div>
</div>

<div class="row">
  <div class="col-xs-3">
    <a href="https://fstrub95.github.io/">
      <img class="people-pic" src="{{ "/static/img/people/florianstrub.png" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://fstrub95.github.io/">Florian Strub</a>
      <h6>University of Lille, Inria | DeepMind</h6>
    </div>
  </div>

  <div class="col-xs-3">
    <a href="http://www-etud.iro.umontreal.ca/~devries/">
      <img class="people-pic" src="{{ "/static/img/people/harmdevries.jpg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="http://www-etud.iro.umontreal.ca/~devries/">Harm de Vries</a>
      <h6>University of Montreal</h6>
    </div>
  </div>

  <div class="col-xs-3">
    <a href="http://wijmans.xyz/">
      <img class="people-pic" src="{{ "/static/img/people/erikwijmans.jpg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="http://wijmans.xyz/">Erik Wijmans</a>
      <h6>Georgia Tech</h6>
    </div>
  </div>

  <div class="col-xs-3">
    <a href="https://www.linkedin.com/in/drew-a-hudson">
      <img class="people-pic" src="{{ "/static/img/people/drew.jpg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://www.linkedin.com/in/drew-a-hudson">Drew Hudson</a>
      <h6>Stanford University</h6>
    </div>
  </div>

  <div class="col-xs-3">
    <a href="http://alanesuhr.com/">
      <img class="people-pic" src="{{ "/static/img/people/alane.png" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="http://alanesuhr.com/">Alane Suhr</a>
      <h6>Cornell University</h6>
    </div>
  </div>

  <div class="col-xs-3">
    <a href="https://abhishekdas.com">
      <img class="people-pic" src="{{ "/static/img/people/abhishek-das-dp.jpg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://abhishekdas.com">Abhishek Das</a>
      <h6>Georgia Tech</h6>
    </div>
  </div>

  <div class="col-xs-3">
    <a href="https://www.cc.gatech.edu/~slee3191/">
      <img class="people-pic" src="{{ "/static/img/people/stefan.png" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://www.cc.gatech.edu/~slee3191/">Stefan Lee</a>
      <h6>Oregon State University</h6>
    </div>
  </div>

<hr>
<div class="row" id="Scientific Committee">
  <div class="col-xs-12">
    <h2>Organizers</h2>
  </div>
</div>

  <div class="col-xs-3">
    <a href="http://www.mateuszmalinowski.com/">
      <img class="people-pic" src="{{ "/static/img/people/mateusz-malinowski-dp.jpeg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="http://www.mateuszmalinowski.com/">Mateusz Malinowski</a>
      <h6>DeepMind</h6>
    </div>
  </div>

  <div class="col-xs-3">
    <a href="https://mila.quebec/en/person/aaron-courville/">
      <img class="people-pic" src="{{ "/static/img/people/aaron-courville-dp.jpg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://mila.quebec/en/person/aaron-courville/">Aaron Courville</a>
      <h6>University of Montreal</h6>
    </div>
  </div>

  <div class="col-xs-3">
    <a href="http://www.lifl.fr/~pietquin/">
      <img class="people-pic" src="{{ "/static/img/people/olivier-pietquin-dp.jpg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="http://www.lifl.fr/~pietquin/">Olivier Pietquin</a>
      <h6>Google Brain</h6>
    </div>
  </div>

</div>
<hr>

<div class="row">
  <div class="col-xs-12">
    <h2>Previous sessions</h2>
  </div>
</div>
<a name="/prev_session"></a>
<div class="row">
  <div class="col-xs-12">
    <p>
      <a href="https://nips2017vigil.github.io/">2017</a>,
      <a href="https://nips2018vigil.github.io/">2018</a>
    </p>
  </div>
</div>
<br>

<div class="row">
  <div class="col-xs-12">
    <h2>References</h2>
  </div>
</div>
<div class="row">
  <div class="col-md-12">
    <ol>
    <li>
     Sahar Kazemzadeh et al. "ReferItGame: Referring to Objects in Photographs of Natural Scenes." EMNLP, 2014.
    </li>
    <li>
 Harm de Vries et al. "GuessWhat?! Visual object discovery through multi-modal dialogue." CVPR, 2017.
</li>
<li>
 Stanislaw Antol et al. "Vqa: Visual question answering." ICCV, 2015.
</li>
<li>
 Mateusz Malinowski et al. "Ask Your Neurons: A Neural-based Approach to Answering Questions about Images." ICCV, 2015.
 </li>
 <li>
 Mateusz Malinowski et al. "A Multi-World Approach to Question Answering about Real-World Scenes based on Uncertain Input." NIPS, 2014.
 </li>

 <li>
 Geman Donald, et al. "Visual Turing test for computer vision systems." PNAS, 2015.
 </li>
 <li>
 Abhishek Das et al. "Visual dialog." CVPR, 2017.
 </li>
 <li>
 Harm de Vries et al. "GuessWhat?! Visual object discovery through multi-modal dialogue." CVPR, 2017.
 </li>
 <li>
 Anna Rohrbach et al. "Generating Descriptions with Grounded and Co-Referenced People." CVPR, 2017.
 </li>
 <li>
 Charles Beattie et al. Deepmind lab. arXiv, 2016.
 </li>
 <li>
 Haonan Yu et al. "Guided Feature Transformation (GFT): A Neural Language Grounding Module for Embodied Agents." arXiv, 2018.
 </li>
 <li>
 Openai universe. https://universe.openai.com, 2016.
 </li>
 <li>
 Alison Gopnik et al. "Semantic and cognitive development in 15- to 21-month-old children." Journal of Child Language, 1984.
 </li>
 <li>
 Abhishek Das et al. "Learning Cooperative Visual Dialog Agents with Deep Reinforcement Learning." ICCV, 2017.
 </li>
 <li>
 Karl Moritz Hermann et al. "Grounded Language Learning in a Simulated 3D World." arXiv, 2017.
 </li>
 <li>
 Alexander G. Huth et al. "Natural speech reveals the semantic maps that tile human cerebral cortex." Nature, 2016.
 </li>
 <li>
 Alexander G. Huth et al. "Decoding the semantic content of natural movies from human brain activity." Frontiers in systems neuroscience, 2016.
 </li>
 <li>
 Piotr Mirowski et al. "Learning to Navigate in Cities Without a Map." NeurIPS, 2018.
 </li>
 <li>
  Karl Moritz Hermann et al. "Learning to Follow Directions in StreetView." arXiv, 2019. 
  </li>
 <li>
 Stevan Harnad. "The symbol grounding problem." CNLS, 1989.
 </li>
 <li>
 E Kolve et al. "AI2-THOR: An Interactive 3D Environment for Visual AI." arXiv, 2017.
 </li>
 <li>
 Yi Wu et al. "House3D: A Rich and Realistic 3D Environment." arXiv, 2017.
 </li>
 <li>
 Angel Chang et al. "Matterport3D: Learning from RGB-D Data in Indoor Environments." arXiv, 2017.
 </li>
 <li>
 Abhishek Das et al. "Embodied Question Answering." CVPR, 2018.
 </li>
 <li>
 Peter Anderson et al. "Vision-and-Language Navigation: Interpreting visually-grounded navigation instructions in real environments." CVPR, 2018.
 </li>
 <li>
 Xin Wang et al. "Reinforced Cross-Modal Matching and Self-Supervised Imitation Learning for Vision-Language Navigation." CVPR, 2019.
  </li>
 <li>
 Fei Xia et al. "Gibson Env: Real-World Perception for Embodied Agents." CVPR, 2018.
 </li>
 <li>
 Manolis Savva et al. "MINOS: Multimodal indoor simulator for navigation in complex environments." arXiv, 2017.
 </li>
 <li>
 Daniel Gordon et al. "IQA: Visual Question Answering in Interactive Environments." CVPR, 2018.
 </li>
 <li>
 Relja Arandjelovic et al. "Look, Listen and Learn." ICCV, 2017.
 </li>
    </ol>
  </div>
</div>
