---
layout: project
urltitle:  "Visually Grounded Interaction and Language (ViGIL)"
title: "Visually Grounded Interaction and Language (ViGIL)"
categories: nips, montreal, canada, workshop, computer vision, natural language, grounding, interaction, machine learning, vigil, 2018, nips18
permalink: /
<!-- favicon: /static/img/embodiedqa/favicon.png -->
bibtex: true
paper: true
acknowledgements: ""
---

<br>
<div class="row">
  <div class="col-xs-12">
    <center><h1>Visually Grounded Interaction and Language (ViGIL)</h1></center>
    <center><h2>NeurIPS 2018 Workshop, Montreal, Canada</h2></center>
    <center><span style="color:#e74c3c;font-weight:400;">
      Friday, 7th December, 08:30 AM to 06:30 PM,
      Room: 512 CDGH
    </span></center>
  </div>
</div>

<hr>

<!-- <div class="row" id="intro"> -->
  <!-- <div class="col-md-12"> -->
    <!-- <img src="{{ "/static/img/splash.png" | prepend:site.baseurl }}"> -->
    <!-- <p> Image credit: [2, 28, 12, 11, 15-21, 26]</p> -->
  <!-- </div> -->
<!-- </div> -->

<br>
<div class="row" id="intro">
  <div class="col-xs-12">
    <h2>Introduction</h2>
  </div>
</div>
<div class="row">
  <div class="col-xs-12">
    <p>
    The dominant paradigm in modern natural language understanding is learning statistical language models from text-only corpora. This approach is founded on a distributional notion of semantics, i.e. that the "meaning" of a word is based only on its relationship to other words. While effective for many applications, methods in this family suffer from limited semantic understanding, as they miss learning from the multimodal and interactive environment in which communication often takes place - the symbols of language thus are not grounded in anything concrete. The symbol grounding problem first highlighted this limitation, that "meaningless symbols (i.e.) words cannot be grounded in anything but other meaningless symbols" [18].
</p>

<p>
On the other hand, humans acquire language by communicating about and interacting within a rich, perceptual environment. This behavior provides the necessary grounding for symbols, i.e. to concrete objects or concepts (i.e. physical or psychological). Thus, recent work has aimed to bridge vision, interactive learning, and natural language understanding through language learning tasks based on natural images (ReferIt [1], GuessWhat?! [2], Visual Question Answering [3,4,5,6], Visual Dialog [7], Captioning [8]) or through embodied agents performing interactive tasks [13,14,17,22,23,24,26] in physically simulated environments (DeepMind Lab [9], Baidu XWorld [10], OpenAI Universe [11], House3D [20], Matterport3D [21], GIBSON [24], MINOS [35], AI2-THOR [19], StreetLearn [17]), often drawing on the recent successes of deep learning and reinforcement learning. We believe this line of research poses a promising, long-term solution to the grounding problem faced by current, popular language understanding models.
</p>


<p>
While machine learning research exploring visually-grounded language learning may be in its earlier stages, it may be possible to draw insights from the rich research literature on human language acquisition. In neuroscience, recent progress in fMRI technology has enabled to better understand the interleave between language, vision and other modalities [15,16] suggesting that the brains shares neural representation of concepts across vision and language. Differently, developmental cognitive scientists have also argued that children acquiring various words is closely linked to them learning the underlying concept in the real world [12].
</p>

<p>
This workshop thus aims to gather people from various backgrounds - machine learning, computer vision, natural language processing, neuroscience, cognitive science, psychology, and philosophy - to share and debate their perspectives on why grounding may (or may not) be important in building machines that truly understand natural language.

</p>
  </div>
</div> <br>

<div class="row" id="dates">
  <div class="col-xs-12">
    <h2>Important Dates</h2>
  </div>
</div>

<div class="row">
  <div class="col-xs-12">
    <table class="table table-striped">
      <tbody>
        <tr>
          <td>Paper Submission Deadline</td>
          <td>November 1st, 2018 - Midnight Pacific Time</td>
        </tr>
        <tr>
          <td>Final Decisions</td>
          <td>November <strike>8</strike> 9th, 2018 </td>
        </tr>
        <tr>
          <td>Workshop Date</td>
          <td>December 7th, 2018</td>
        </tr>
      </tbody>
    </table>
  </div>
</div><br>

<div class="row" id="cfp">
  <div class="col-xs-12">
    <h2>Call for Papers</h2>
  </div>
</div>
<div class="row">
  <div class="col-xs-12">
    <p>
      We invite high-quality paper submissions on the following topics:
    </p>

  <p>
  <ul>
  <li>language acquisition or learning through interactions</li>
  <li>visual captioning, dialog, and question-answering</li>
  <li>reasoning in language and vision</li>
  <li>visual synthesis from language</li>
  <li>transfer learning in language and vision tasks</li>
  <li>navigation in virtual worlds with natural-language instructions</li>
  <li>machine translation with visual cues</li>
  <li>novel tasks that combine language, vision and actions</li>
  <li>understanding and modeling the relationship between language and vision in humans</li>
  <li>semantic systems and modeling of natural language and visual stimuli representations in the human brain</li>
  <li> audio visual scene-aware dialog systems - Visual Question Answering</li>
  <li> image/video captioning</li>
  <li> lip reading</li>
  <li> audio-visual scene understanding - Sound localization </li>
  <li> audio-visual speech processing </li>
  <li> audio-visual fusion </li>
</ul>
    </p>


  <p> <span style="font-weight:500;">Submission:</span>
  <br/>
    Accepted papers
    will be presented during joint poster sessions, with exceptional submissions
    selected for spotlight oral presentation. Accepted papers will be made publicly
    available as non-archival reports,
    allowing future submissions to archival conferences or journals.
  </p>

  <p>
  Submissions should be up to 4 pages excluding references, acknowledgements, and supplementary material, and should be
      <span style="color:#1a1aff;font-weight:400;"><a href="https://nips.cc/Conferences/2018/PaperInformation/StyleFiles">NIPS format</a></span> and anonymous. The review process is double-blind.
  </p>
  <p>
  We also welcome published papers that are within the scope of the workshop (without re-formatting). This specific papers do not have to be anonymous. They are eligible for poster sessions and will only have a very light review process.
  </p>

  <p>
  Please submit your paper to the following address: <a href="https://cmt3.research.microsoft.com/VIGIL2018">https://cmt3.research.microsoft.com/VIGIL2018</a>
  If you have any question, send an email to: vigilworkshop2018@gmail.com
  </p>

  <p><b>Accepted workshop papers are eligible to the pool of reserved conference tickets (one ticket per accepted papers).</b>
  </p>

  </div>

</div><br>




<div class="row" id="schedule">
  <div class="col-xs-12">
    <h2>Schedule</h2>
  </div>
</div>

<div class="row">
  <div class="col-xs-12">
    <ul>
      <li>08:30 AM : Opening Remarks                        </li>
      <li>08:40 AM : Invited Speaker 1: Steven Harnad - The symbol grounding problem  | <a href="https://github.com/nips2018vigil/nips2018vigil.github.io/blob/master/static/slides/harnad.pptx?raw=true">Slides</a> </li>
      <li>09:20 AM : Invited Speaker 2: Antonio Torralba - Learning to See and Hear    </li>
      <li>10:00 AM : Audio Visual Semantic Understanding Challenge </li>
      <li>10:15 AM : Spotlights (6*2min)
      <ul>
        <li>A Distributional Semantic Model of Visually Indirect Grounding for Abstract Words- <i>Akira Utsumi</i>  <span style="color:#1a1aff;font-weight:400;">[<a href="{{ "/static/papers/accepted/36.pdf" | prepend:site.baseurl }}">pdf</a>]</span></li>
        <li>Visual Reasoning by Progressive Module Network- <i>SeungWook Kim, Makarand Tapaswi, Sanja Fidler</i>  <span style="color:#1a1aff;font-weight:400;">[<a href="{{ "/static/papers/accepted/29.pdf" | prepend:site.baseurl }}">pdf</a>]</span></li>
        <li>Generating Diverse Programs with Instruction Conditioned Reinforced Adversarial Learning- <i>Aishwarya Agrawal, Mateusz Malinowski, Felix Hill, Ali Eslami, Oriol Vinyals, Tejas Kulkarni</i>  <span style="color:#1a1aff;font-weight:400;">[<a href="{{ "/static/papers/accepted/22.pdf" | prepend:site.baseurl }}">pdf</a>]</span></li>
        <li>Learning to Caption Images by Asking Natural Language Questions- <i>Kevin Shen, Amlan Kar, Sanja Fidler</i>  <span style="color:#1a1aff;font-weight:400;">[<a href="{{ "/static/papers/accepted/18.pdf" | prepend:site.baseurl }}">pdf</a>]</span></li>
        <li>Generating Animated Videos of Human Activities from Natural Language Descriptions - <i>Angela S. Lin, Lemeng Wu, Rodolfo Corona, Kevin Tai, Qixing Huang, Raymond J. Mooney</i> <span style="color:#1a1aff;font-weight:400;">[<a href="{{ "/static/papers/accepted/9.pdf" | prepend:site.baseurl }}">pdf</a>]</span></li>
        <li>Multimodal Abstractive Summarization for Open-Domain Videos - <i>Jindrich Libovicky, Shruti Palaskar, Spandana Gella, Florian Metze</i> <span style="color:#1a1aff;font-weight:400;">[<a href="{{ "/static/papers/accepted/8.pdf" | prepend:site.baseurl }}">pdf</a>]</span></li>
        <!-- <li></li> -->
      </ul>
      </li>
      <li>10:30 AM : Coffee Break (20min)                   </li>
      <li>10:50 AM : Invited Speaker 3: Douwe Kiela - Learning Multimodal Embeddings   </li>
      <li>11:30 AM : Invited Speaker 4: Roozbeh Mottaghi - Interactive Scene Understanding </li>
      <li>12:10 PM : Poster Session <b>Lunch provided!</b> </li>
      <li>01:10 PM : Break                                  </li>
      <li>01:40 PM : Invited Speaker 5: Angeliki Lazaridou - Emergence of (linguistic communication) through multi-agent interactions </li>
      <li>02:20 PM : Invited Speaker 6: Barbara Landau - Learning simple spatial terms: Core and more     </li>
      <li>03:00 PM : Coffee Break & Poster Session (50 mins)</li>
      <li>03:50 PM : Invited Speaker 7: Joyce Y. Chai - Language Communication with Robots </li>
      <li>04:30 PM : Invited Speaker 8: Christopher Manning - Towards real-world visual reasoning | <a href="https://stanford.app.box.com/s/5ce8qqylqj41ytnyxzme4mq9vx587mm6">Slides</a></li>
      <li>05:10 PM : Panel Discussion                       </li>
      <li>06:00 PM : Closing Remarks                        </li>
    </ul>
  </div>
</div>


<br>
<div class="row" id="speakers">
  <div class="col-xs-12">
    <h2>Invited Speakers</h2>
  </div>
</div><br>

<div class="row">
  <div class="col-md-12">
    <img class="speaker-pic" style="float:left;margin-right:50px;" src="{{ "/static/img/people/stevanharnad.png" | prepend:site.baseurl }}">
    <p><b>Stevan Harnad</b>
    is Professor of Cognitive Sciences in the Department of Psychology at Université du Québec à Montréal, and Professor of Web Science in the Department of Electronics and Computer Science, at University of Southampton, UK.
    His research is on categorisation, communication, cognition and the open research web.
    <span style="color:#1a1aff;font-weight:400;">[<a href="https://www.ecs.soton.ac.uk/people/harnad">Webpage</a>]</span></p>
  </div>
</div><br>

<div class="row">
  <div class="col-md-12">
    <img class="speaker-pic" style="float:left;margin-right:50px;" src="{{ "/static/img/people/barbaralandau.jpg" | prepend:site.baseurl }}">
    <p><b>Barbara Landau</b> is Professor of Cognitive Science at Johns Hopkins University. Landau is interested in human knowledge of language and space, and the relationships between these two foundational systems of knowledge.
    <span style="color:#1a1aff;font-weight:400;">[<a href="http://cogsci.jhu.edu/directory/barbara-landau/">Webpage</a>]</span></p>
  </div>
</div><br>

<div class="row">
  <div class="col-md-12">
    <img class="speaker-pic" style="float:left;margin-right:50px;" src="{{ "/static/img/people/joycechai.jpeg" | prepend:site.baseurl }}">
    <p><b>Joyce Chai</b>
    is a Professor at Michigan State University.  Her lab investigates language use in a variety of context and develops approaches for situated language processing to faciliate situated communication with artificial agents.
    <span style="color:#1a1aff;font-weight:400;">[<a href="http://www.cse.msu.edu/~jchai/">Webpage</a>]</span></p>
  </div>
</div><br>

<div class="row">
  <div class="col-md-12">
    <img class="speaker-pic" style="float:left;margin-right:50px;" src="{{ "/static/img/people/christophermanning.jpg" | prepend:site.baseurl }}">
    <p><b>Christopher Manning</b>
    is the inaugral Thomas M. Siebel Professor in Machine Learning in the Departments of Computer Science and Linguistics at Stanford University. His research goal is computers that can intelligently process, understand, and generate human language material.
    <span style="color:#1a1aff;font-weight:400;">[<a href="https://nlp.stanford.edu/manning/">Webpage</a>]</span></p>
  </div>
</div><br>

<div class="row">
  <div class="col-md-12">
    <img class="speaker-pic" style="float:left;margin-right:50px;" src="{{ "/static/img/people/antoniotorralba.jpg" | prepend:site.baseurl }}">
    <p><b>Antonio Torralba</b>
    is a Professor at Massachusetts Institute of Technology. He is  interested in building systems that can perceive the world like humans do. Although his work focuses on computer vision, he is also interested in other modalities such as audition and touch.
    <span style="color:#1a1aff;font-weight:400;">[<a href="http://web.mit.edu/torralba/www/">Webpage</a>]</span></p>
  </div>
</div><br>

<div class="row">
  <div class="col-md-12">
    <img class="speaker-pic" style="float:left;margin-right:50px;" src="{{ "/static/img/people/roozbeh_mottaghi.jpg" | prepend:site.baseurl }}">
    <p><b>Roozbeh Mottaghi</b>
    is a Research Scientist at the Allen Institute for Artificial Intelligence.  His research interests include
    computer vision, reasoning, natural language, and action.
    <span style="color:#1a1aff;font-weight:400;">[<a href="https://cs.stanford.edu/~roozbeh/">Webpage</a>]</span></p>
  </div>
</div><br>

<div class="row">
  <div class="col-md-12">
    <img class="speaker-pic" style="float:left;margin-right:50px;" src="{{ "/static/img/people/douwekiela.jpg" | prepend:site.baseurl }}">
    <p><b>Douwe Kiela</b>
    is research scientist at Facebook AI Research in New York. His research interests
    are in natural language processing, semantics, grounding, computer vision, deep learning and artificial general intelligence.
    <span style="color:#1a1aff;font-weight:400;">[<a href="https://www.cl.cam.ac.uk/~dk427/">Webpage</a>]</span></p>
  </div>
</div><br>

<div class="row">
  <div class="col-md-12">
    <img class="speaker-pic" style="float:left;margin-right:50px;" src="{{ "/static/img/people/angelikilazaridou.jpg" | prepend:site.baseurl }}">
    <p><b>Angeliki Lazaridou</b>
    is  a research scientist at DeepMind. Her primary research interests are in the area of natural language processing (NLP), and specifically, in multimodal semantics.
    <span style="color:#1a1aff;font-weight:400;">[<a href="http://angelikilazaridou.github.io/research/">Webpage</a>]</span></p>
  </div>
</div><br>

<div class="row" id="recordings">
    <div class="col-xs-12">
    <h2>Recordings</h2>
  </div>
</div>

<div class="row">
  <div class="col-xs-12">
    <p>
    The workshop was broadcasted via BlueJeans. You can find the recordings here: 
    <a href="https://bluejeans.com/s/KAsSh">Morning Session</a> | <a href="https://bluejeans.com/s/hEo5B">Afternoon Session</a></p>
  </div>
</div>

<div class="row" id="accepted-papers">
  <div class="col-xs-12">
    <h2>Accepted Papers</h2>
  </div>
</div>

<div class="row">
<div class="col-xs-12">
  <ul>
    <li>Modelling Visual Properties and Visual Context in Multimodal Semantics - <i>Christopher Davis, Luana Bulat, Anita Vero, Ekaterina Shutova</i> <span style="color:#1a1aff;font-weight:400;">[<a href="{{ "/static/papers/accepted/1.pdf" | prepend:site.baseurl }}">pdf</a>]</span></li>

    <li>Guiding Policies with Language via Meta-Learning - <i>John D. Co-Reyes, Abhishek Gupta, Suvansh Sanjeev, Nick Altieri, John DeNero, Pieter Abbeel, Sergey Levine</i> <span style="color:#1a1aff;font-weight:400;">[<a href="{{ "/static/papers/accepted/2.pdf" | prepend:site.baseurl }}">pdf</a>]</span></li>

    <li>Language coverage and generalization in RNN-based continuous sentence embeddings for interacting agents - <i>Luca Celotti, Simon Brodeur, Jean Rouat</i> <span style="color:#1a1aff;font-weight:400;">[<a href="{{ "/static/papers/accepted/3.pdf" | prepend:site.baseurl }}">pdf</a>]</span></li>

    <li>Scene Graph Parsing by Attention Graph - <i>Martin Andrews, Yew Ken Chia, Sam Witteveen</i>  <span style="color:#1a1aff;font-weight:400;">[<a href="{{ "/static/papers/accepted/4.pdf" | prepend:site.baseurl }}">pdf</a>]</span></li>

    <li>Visual Entailment Task for Visually-Grounded Language Learning - <i>Ning Xie, Farley Lai, Derek Doran, Asim Kadav</i> <span style="color:#1a1aff;font-weight:400;">[<a href="{{ "/static/papers/accepted/5.pdf" | prepend:site.baseurl }}">pdf</a>]</span></li>

    <li>On transfer learning using a MAC model variant - <i>Vincent Marois, T.S. Jayram, Vincent Albouy, Tomasz Kornuta, Younes Bouhadjar, Ahmet S. Ozcan</i> <span style="color:#1a1aff;font-weight:400;">[<a href="{{ "/static/papers/accepted/6.pdf" | prepend:site.baseurl }}">pdf</a>]</span></li>

    <li>Learning Capsule Networks with Images and Text - <i>Yufei Feng, Xiaodan Zhu, Yifeng Li, Yuping Ruan, Michael Greenspan</i> <span style="color:#1a1aff;font-weight:400;">[<a href="{{ "/static/papers/accepted/7.pdf" | prepend:site.baseurl }}">pdf</a>]</span></li>

    <li>Multimodal Abstractive Summarization for Open-Domain Videos - <i>Jindrich Libovicky, Shruti Palaskar, Spandana Gella, Florian Metze</i> <span style="color:#1a1aff;font-weight:400;">[<a href="{{ "/static/papers/accepted/8.pdf" | prepend:site.baseurl }}">pdf</a>]</span></li>

    <li>Generating Animated Videos of Human Activities from Natural Language Descriptions - <i>Angela S. Lin, Lemeng Wu, Rodolfo Corona, Kevin Tai, Qixing Huang, Raymond J. Mooney</i> <span style="color:#1a1aff;font-weight:400;">[<a href="{{ "/static/papers/accepted/9.pdf" | prepend:site.baseurl }}">pdf</a>]</span></li>

    <li>Mixture of Regression Experts in fMRI Encoding - <i>Subba Reddy Oota, Adithya Avvaru, Naresh Mawani, Raju S. Bapi</i> <span style="color:#1a1aff;font-weight:400;">[<a href="{{ "/static/papers/accepted/10.pdf" | prepend:site.baseurl }}">pdf</a>]</span></li>

    <li>TOUCHDOWN: Natural Language Navigation and Spatial Reasoning in Visual Street Environments - <i>Howard Chen, Alane Suhr, Dipendra Misra, Yoav Artzi</i> <span style="color:#1a1aff;font-weight:400;">[<a href="{{ "/static/papers/accepted/11.pdf" | prepend:site.baseurl }}">pdf</a>]</span></li>

    <li>Following Formulaic Map Instructions in a Street Simulation Environment - <i>Volkan Cirik, Yuan Zhang, Jason Baldridge</i> <span style="color:#1a1aff;font-weight:400;">[<a href="{{ "/static/papers/accepted/12.pdf" | prepend:site.baseurl }}">pdf</a>]</span></li>

    <li>Keep Drawing It: Iterative language-based image generation and editing- <i>Alaaeldin El-Nouby, Shikhar Sharma, Hannes Schulz, Devon Hjelm, Layla El Asri, Samira Ebrahimi Kahou, Yoshua Bengio, Graham W. Taylor</i>  <span style="color:#1a1aff;font-weight:400;">[<a href="{{ "/static/papers/accepted/13.pdf" | prepend:site.baseurl }}">pdf</a>]</span></li>

    <li>Blindfold Baselines for Embodied QA - <i>Ankesh Anand, Eugene Belilovsky, Kyle Kastner, Hugo Larochelle, Aaron Courville</i>  <span style="color:#1a1aff;font-weight:400;">[<a href="{{ "/static/papers/accepted/14.pdf" | prepend:site.baseurl }}">pdf</a>]</span></li>

    <li>Object-oriented Targets for Visual Navigation using Rich Semantic Representations- <i>Jean-Benoit Delbrouck, Stéphane Dupont</i>  <span style="color:#1a1aff;font-weight:400;">[<a href="{{ "/static/papers/accepted/15.pdf" | prepend:site.baseurl }}">pdf</a>]</span></li>

    <li>CLEAR: A Dataset for Compositional Language and Elementary Acoustic Reasoning- <i>Jerome Abdelnour, Giampiero Salvi, Jean Rouat</i>  <span style="color:#1a1aff;font-weight:400;">[<a href="{{ "/static/papers/accepted/16.pdf" | prepend:site.baseurl }}">pdf</a>]</span></li>

    <li>Incremental Object Model Learning from Multimodal Human-Robot Interactions- <i></i>  <span style="color:#1a1aff;font-weight:400;">[<a href="{{ "/static/papers/accepted/17.pdf" | prepend:site.baseurl }}">pdf</a>]</span></li>

    <li>Learning to Caption Images by Asking Natural Language Questions- <i>Kevin Shen, Amlan Kar, Sanja Fidler</i>  <span style="color:#1a1aff;font-weight:400;">[<a href="{{ "/static/papers/accepted/18.pdf" | prepend:site.baseurl }}">pdf</a>]</span></li>

    <li>Audio-Visual Fusion for Sentiment Classification using Cross-Modal Autoencoder- <i>Sri Harsha Dumpala, Imran Sheikh, Rupayan Chakraborty, Sunil Kumar Kopparapu</i>  <span style="color:#1a1aff;font-weight:400;">[<a href="{{ "/static/papers/accepted/19.pdf" | prepend:site.baseurl }}">pdf</a>]</span></li>

    <li>Compositional Hard Negatives for Visual Semantic Embeddings via an Adversary- <i>Avishek Joey Bose, Huan Ling, Yanshuai Cao</i>  <span style="color:#1a1aff;font-weight:400;">[<a href="{{ "/static/papers/accepted/20.pdf" | prepend:site.baseurl }}">pdf</a>]</span></li>

    <li>SARN: Relational Reasoning through Sequential Attention- <i>Jinwon An, Sungwon Lyu, Sungzoon Cho</i>  <span style="color:#1a1aff;font-weight:400;">[<a href="{{ "/static/papers/accepted/21.pdf" | prepend:site.baseurl }}">pdf</a>]</span></li>

    <li>Generating Diverse Programs with Instruction Conditioned Reinforced Adversarial Learning- <i>Aishwarya Agrawal, Mateusz Malinowski, Felix Hill, Ali Eslami, Oriol Vinyals, Tejas Kulkarni</i>  <span style="color:#1a1aff;font-weight:400;">[<a href="{{ "/static/papers/accepted/22.pdf" | prepend:site.baseurl }}">pdf</a>]</span></li>

    <li>Large-Scale Answerer in Questioner’s Mind for Visual Dialog Question Generation- <i>Sang-Woo Lee, Tong Gao, Sohee Yang, Jaejun Yoo, Jung-Woo Ha</i>  <span style="color:#1a1aff;font-weight:400;">[<a href="{{ "/static/papers/accepted/23.pdf" | prepend:site.baseurl }}">pdf</a>]</span></li>

    <li>Variational learning across domains with triplet information- <i>Rita Kuznetsova, Oleg Bakhteev, Alexandr Ogaltsov</i>  <span style="color:#1a1aff;font-weight:400;">[<a href="{{ "/static/papers/accepted/24.pdf" | prepend:site.baseurl }}">pdf</a>]</span></li>

    <li>Product Title Refinement via Multi-Modal Generative Adversarial Learning- <i>Jian-Guo Zhang, Pengcheng Zou, Zhao Li, Yao Wan, Ye Liu, Xiuming Pan, Yu Gong, Philip S. Yu</i>  <span style="color:#1a1aff;font-weight:400;">[<a href="{{ "/static/papers/accepted/25.pdf" | prepend:site.baseurl }}">pdf</a>]</span></li>

    <li>How2: A Large-scale Dataset for Multimodal Language Understanding- <i>Ramon Sanabria, Ozan Caglayan, Shruti Palaskar, Desmond Elliott, Loic Barrault, Lucia Specia, Florian Metze</i>  <span style="color:#1a1aff;font-weight:400;">[<a href="{{ "/static/papers/accepted/26.pdf" | prepend:site.baseurl }}">pdf</a>]</span></li>

    <li>Generating Diverse and Accurate Visual Captions by Comparative Adversarial Learning- <i>Dianqi Li, Qiuyuan Huang, Xiaodong He, Lei Zhang, Ming-Ting Sun</i>  <span style="color:#1a1aff;font-weight:400;">[<a href="{{ "/static/papers/accepted/27.pdf" | prepend:site.baseurl }}">pdf</a>]</span></li>

    <li>Zero-Shot Image Classification Guided by Natural Language Descriptions of Classes: A Meta-Learning Approach- <i>R. Lily Hu, Caiming Xiong, Richard Socher</i>  <span style="color:#1a1aff;font-weight:400;">[<a href="{{ "/static/papers/accepted/28.pdf" | prepend:site.baseurl }}">pdf</a>]</span></li>

    <li>Visual Reasoning by Progressive Module Network- <i>SeungWook Kim, Makarand Tapaswi, Sanja Fidler</i>  <span style="color:#1a1aff;font-weight:400;">[<a href="{{ "/static/papers/accepted/29.pdf" | prepend:site.baseurl }}">pdf</a>]</span></li>

    <li>Scene Graphs for Interpretable Video Anomaly Classification- <i>Nicholas F. Y. Chen, Zhiyuan Du, Khin Hua Ng</i>  <span style="color:#1a1aff;font-weight:400;">[<a href="{{ "/static/papers/accepted/30.pdf" | prepend:site.baseurl }}">pdf</a>]</span></li>

    <li>Learning Unsupervised Visual Grounding Through Semantic Self-Supervision- <i>Syed Ashar Javed, Shreyas Saxena, Vineet Gandhi</i>  <span style="color:#1a1aff;font-weight:400;">[<a href="{{ "/static/papers/accepted/31.pdf" | prepend:site.baseurl }}">pdf</a>]</span></li>

    <li>Towards Audio to Scene Image Synthesis using Generative Adversarial Network- <i>Chia-Hung, Wan, Shun-Po, Chuang, Hung-Yi, Lee</i>  <span style="color:#1a1aff;font-weight:400;">[<a href="{{ "/static/papers/accepted/32.pdf" | prepend:site.baseurl }}">pdf</a>]</span></li>

    <li>Attend and Attack: Attention Guided Adversarial Attacks on Visual Question Answering Models- <i>Vasu Sharma, Ankita Kalra, Vaibhav , Simral Chaudhary, Labhesh Patel, LP Morency</i>  <span style="color:#1a1aff;font-weight:400;">[<a href="{{ "/static/papers/accepted/33.pdf" | prepend:site.baseurl }}">pdf</a>]</span></li>

    <li>An Interpretable Model for Scene Graph Generation- <i>Ji Zhang, Kevin Shih, Andrew Tao, Bryan Catanzaro, Ahmed Elgammal</i>  <span style="color:#1a1aff;font-weight:400;">[<a href="{{ "/static/papers/accepted/34.pdf" | prepend:site.baseurl }}">pdf</a>]</span></li>

    <li>A Corpus for Reasoning About Natural Language Grounded in Photographs- <i>Alane Suhrz, Stephanie Zhouy, Iris Zhangz, Huajun Baiz, Yoav Artziz</i>  <span style="color:#1a1aff;font-weight:400;">[<a href="{{ "/static/papers/accepted/35.pdf" | prepend:site.baseurl }}">pdf</a>]</span></li>

    <li>A Distributional Semantic Model of Visually Indirect Grounding for Abstract Words- <i>Akira Utsumi</i>  <span style="color:#1a1aff;font-weight:400;">[<a href="{{ "/static/papers/accepted/36.pdf" | prepend:site.baseurl }}">pdf</a>]</span></li>

    <li>Efficient Visual Dialog Policy Learning via Positive Memory Retention- <i>Rui Zhao, Volker Tresp</i>  <span style="color:#1a1aff;font-weight:400;">[<a href="{{ "/static/papers/accepted/37.pdf" | prepend:site.baseurl }}">pdf</a>]</span></li>

    <li>Choose Your Neuron: Incorporating Domain Knowledge through Neuron-Importance - <i>Ramprasaath R. Selvaraju, Prithvijit Chattopadhyay, Mohamed Elhoseiny, Tilak Sharma, Dhruv Batra, Devi Parikh, Stefan Lee</i>  <span style="color:#1a1aff;font-weight:400;">[<a href="{{ "/static/papers/accepted/38.pdf" | prepend:site.baseurl }}">pdf</a>]</span></li>

    <li>A Bayesian Approach to Phrase Understanding through Cross-Situational Learning - <i>Amir Aly, Tadahiro Taniguchi, Daichi Mochihashi</i>  <span style="color:#1a1aff;font-weight:400;">[<a href="{{ "/static/papers/accepted/39.pdf" | prepend:site.baseurl }}">pdf</a>]</span></li>

    <li>Embodied Question Answering in Photorealistic Environments with Point Cloud Perception - <i>Erik Wijmans*, Samyak Datta*, Oleksandr Maksymets*, Abhishek Das, Georgia Gkioxari, Stefan Lee, Irfan Essa, Devi Parikh, Dhruv Batra</i>  <span style="color:#1a1aff;font-weight:400;">[<a href="{{ "/static/papers/accepted/eqa_mp3d_vigil18.pdf" | prepend:site.baseurl }}">pdf</a>]</span></li>

    <li>Out of the Box: Reasoning with Graph Convolution Nets for Factual Visual Question Answering - <i>Medhini Narasimhan, Svetlana Lazebnik, Alexander Schwing </i>
    <span style="color:#1a1aff;font-weight:400;">[<a href="{{ "/static/papers/accepted/extra-1.pdf" | prepend:site.baseurl }}">pdf</a>]</span>
    </li>

    <li>DVQA: Understanding Data Visualizations via Question Answering - <i>Kushal Kafle, Brian Price, Scott Cohen, Christopher Kanan</i>
    <span style="color:#1a1aff;font-weight:400;">[<a href="{{ "/static/papers/accepted/extra-2.pdf" | prepend:site.baseurl }}">pdf</a>]</span>
    </li>

    <li>Dialog-based Interactive Image Retrieval - <i>Hui Wu, Xiaoxiao Guo, Rogerio Feris, Yu Cheng, Steven Rennie, Gerald Tesauro</i>
    <span style="color:#1a1aff;font-weight:400;">[<a href="{{ "/static/papers/accepted/extra-3.pdf" | prepend:site.baseurl }}">pdf</a>]</span>
    </li>

    <li>TallyQA: Answering Complex Counting Questions - <i>Manoj Acharya, Kushal Kafle, Christopher Kanan</i>
    <span style="color:#1a1aff;font-weight:400;">[<a href="{{ "/static/papers/accepted/extra-4.pdf" | prepend:site.baseurl }}">pdf</a>]</span>
    </li>

    <li>Efficient Gradient Computation for Structured Output Learning with Rational and Tropical Losses - <i>Corinna Cortes, Vitaly Kuznetsov,Mehryar Mohri, Dmitry Storcheus, Scott Yang </i>
    <span style="color:#1a1aff;font-weight:400;">[<a href="{{ "/static/papers/accepted/extra-5.pdf" | prepend:site.baseurl }}">pdf</a>]</span>
    </li>

    <li>Systematic Generalization: What Is Required and Can It Be Learned? - <i>Shikhar Murty, Dzmitry Bahdanau, Michael Noukhovitch, Thien Nguyen, Harm De Vries, Aaron Courville</i>
    <span style="color:#1a1aff;font-weight:400;">[<a href="{{ "/static/papers/accepted/extra-6.pdf" | prepend:site.baseurl }}">pdf</a>]</span>
    </li>
    
    <li>Systematic Generalization: What Is Required and Can It Be Learned? - <i>Pablo Azagra, Ana Cristina Murillo, Manuel Lopes, Javier Civera </i>
    <span style="color:#1a1aff;font-weight:400;">[<a href="{{ "/static/papers/accepted/45.pdf" | prepend:site.baseurl }}">pdf</a>] </span>
    - <span style="color:#1a1aff;font-weight:400;">[<a href="{{ "/static/papers/accepted/45_supp.pdf" | prepend:site.baseurl }}">supp</a>]</span>
    </li>
  </ul>
</div>
</div>

<div class="row" id="organizers">
  <div class="col-xs-12">
    <h2>Organizers</h2>
  </div>
</div>

<div class="row">
  <div class="col-xs-3">
    <a href="https://fstrub95.github.io/">
      <img class="people-pic" src="{{ "/static/img/people/florianstrub.png" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://fstrub95.github.io/">Florian Strub</a>
      <h6>University of Lille, Inria | DeepMind</h6>
    </div>
  </div>

  <div class="col-xs-3">
    <a href="http://www-etud.iro.umontreal.ca/~devries/">
      <img class="people-pic" src="{{ "/static/img/people/harmdevries.jpg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="http://www-etud.iro.umontreal.ca/~devries/">Harm de Vries</a>
      <h6>University of Montreal</h6>
    </div>
  </div>

  <div class="col-xs-3">
    <a href="http://wijmans.xyz/">
      <img class="people-pic" src="{{ "/static/img/people/erikwijmans.jpg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="http://wijmans.xyz/">Erik Wijmans</a>
      <h6>Georgia Tech</h6>
    </div>
  </div>

  <div class="col-xs-3">
    <a href="https://samyak-268.github.io/">
      <img class="people-pic" src="{{ "/static/img/people/samyak.png" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://samyak-268.github.io/">Samyak Datta</a>
      <h6>Georgia Tech</h6>
    </div>
  </div>

  <div class="col-xs-3">
    <a href="http://ethanperez.net/">
      <img class="people-pic" src="{{ "/static/img/people/ethanperez.jpg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="http://ethanperez.net/">Ethan Perez</a>
      <h6>New York University</h6>
    </div>
  </div>

  <div class="col-xs-3">
    <a href="https://www.cc.gatech.edu/~slee3191/">
      <img class="people-pic" src="{{ "/static/img/people/stefan.png" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://www.cc.gatech.edu/~slee3191/">Stefan Lee</a>
      <h6>Georgia Tech</h6>
    </div>
  </div>

  <div class="col-xs-3">
    <a href="http://www.panderson.me/">
      <img class="people-pic" src="{{ "/static/img/people/peter.png" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="http://www.panderson.me/">Peter Anderson</a>
      <h6>Georgia Tech</h6>
    </div>
  </div>

  <div class="col-xs-3">
    <a href="http://www.mateuszmalinowski.com/">
      <img class="people-pic" src="{{ "/static/img/people/mateusz-malinowski-dp.jpeg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="http://www.mateuszmalinowski.com/">Mateusz Malinowski</a>
      <h6>DeepMind</h6>
    </div>
  </div>

  <div class="col-xs-3">
    <a href="https://www.cc.gatech.edu/~dbatra/">
      <img class="people-pic" src="{{ "/static/img/people/dhruv-batra-dp.jpg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://www.cc.gatech.edu/~dbatra/">Dhruv Batra</a>
      <h6>Georgia Tech | Facebook AI Research</h6>
    </div>
  </div>

  <div class="col-xs-3">
    <a href="https://mila.quebec/en/person/aaron-courville/">
      <img class="people-pic" src="{{ "/static/img/people/aaron-courville-dp.jpg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://mila.quebec/en/person/aaron-courville/">Aaron Courville</a>
      <h6>University of Montreal</h6>
    </div>
  </div>

  <div class="col-xs-3">
    <a href="http://www.lifl.fr/~pietquin/">
      <img class="people-pic" src="{{ "/static/img/people/olivier-pietquin-dp.jpg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="http://www.lifl.fr/~pietquin/">Olivier Pietquin</a>
      <h6>Google Brain</h6>
    </div>
  </div>

  <div class="col-xs-3">
    <a href="https://www.cc.gatech.edu/~dparikh/">
      <img class="people-pic" src="{{ "/static/img/people/devi-parikh-dp.jpg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://www.cc.gatech.edu/~dparikh/">Devi Parikh</a>
      <h6>Georgia Tech | Facebook AI Research</h6>
    </div>
  </div>

  <div class="col-xs-3">
    <a href="http://www.merl.com/people/chori">
      <img class="people-pic" src="{{ "/static/img/people/chiori-hori-dp.jpg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="http://www.merl.com/people/chori">Chiori Hori</a>
      <h6>MERL</h6>
    </div>
  </div>

  <div class="col-xs-3">
    <a href="http://users.cecs.anu.edu.au/~cherian/">
      <img class="people-pic" src="{{ "/static/img/people/anoop-cherian-dp.jpg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="http://users.cecs.anu.edu.au/~cherian/">Anoop Cherian</a>
      <h6>MERL</h6>
    </div>
  </div>

  <div class="col-xs-3">
    <a href="http://www.merl.com/people/tmarks">
      <img class="people-pic" src="{{ "/static/img/people/tim-marks-dp.jpg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="http://www.merl.com/people/tmarks">Tim Marks</a>
      <h6>MERL</h6>
    </div>
  </div>

</div>

<hr>



<div class="row">
  <div class="col-xs-12">
    <h2>Sponsors</h2>
  </div>
</div>
<a name="/sponsors"></a>
<div class="row">
  <div class="col-xs-12 sponsor">
    <a href="https://merl.com/">
      <img src="{{ "/static/img/ico/merl-logo-big.jpg" | prepend:site.baseurl }}">
    </a>
    <a href="https://deepmind.com/">
      <img src="{{ "/static/img/ico/deepmind_logo.png" | prepend:site.baseurl }}">
    </a>
        <a href="https://ai.google/">
      <img src="{{ "/static/img/ico/googlelogo.png" | prepend:site.baseurl }}">
    </a>
        <a href="https://iglu-chistera.github.io/">
      <img src="{{ "/static/img/ico/logoIGLU-300.png" | prepend:site.baseurl }}">
    </a>
    <a href="http://uber.ai/">
      <img src="{{ "/static/img/ico/logo_uber.jpg" | prepend:site.baseurl }}">
    </a>
  </div>
</div>
<br>


<hr>

{% if page.acknowledgements %}
<div class="row">
  <div class="col-xs-12">
    <h2>Acknowledgments</h2>
  </div>
</div>
<a name="/acknowledgements"></a>
<div class="row">
  <div class="col-xs-12">
    <p>
      Thanks to <span style="color:#1a1aff;font-weight:400;"> <a href="https://visualdialog.org/">visualdialog.org</a></span> for the webpage format.
    </p>
  </div>
</div>
{% endif %}

<div class="row">
  <div class="col-xs-12">
    <h2>Previous sessions</h2>
  </div>
</div>
<a name="/prev_session"></a>
<div class="row">
  <div class="col-xs-12">
    <p>
      <a href="https://nips2017vigil.github.io/">2017</a>
    </p>
  </div>
</div>

<div class="row">
  <div class="col-xs-12">
    <h2>References</h2>
  </div>
</div>
<div class="row">
  <div class="col-md-12">
    <ol>
    <li>
     Sahar Kazemzadeh et al. "ReferItGame: Referring to Objects in Photographs of Natural Scenes." EMNLP, 2014.
    </li>
    <li>
 Harm de Vries et al. "GuessWhat?! Visual object discovery through multi-modal dialogue." CVPR, 2017.
</li>
<li>
 Stanislaw Antol et al. "Vqa: Visual question answering." ICCV, 2015.
</li>
<li>
 Mateusz Malinowski et al. "Ask Your Neurons: A Neural-based Approach to Answering Questions about Images." ICCV, 2015.
 </li>
 <li>
 Mateusz Malinowski et al. "A Multi-World Approach to Question Answering about Real-World Scenes based on Uncertain Input." NIPS, 2014.
 </li>

 <li>
 Geman Donald, et al. "Visual Turing test for computer vision systems." PNAS, 2015.
 </li>
 <li>
 Abhishek Das et al. "Visual dialog." CVPR, 2017.
 </li>
 <li>
 Anna Rohrbach et al. "Generating Descriptions with Grounded and Co-Referenced People." CVPR, 2017.
 </li>
 <li>
 Charles Beattie et al. Deepmind lab. arXiv, 2016.
 </li>
 <li>
 Haonan Yu et al. "Guided Feature Transformation (GFT): A Neural Language Grounding Module for Embodied Agents." arXiv, 2018.
 </li>
 <li>
 Openai universe. https://universe.openai.com, 2016.
 </li>
 <li>
 Alison Gopnik et al. "Semantic and cognitive development in 15- to 21-month-old children." Journal of Child Language, 1984.
 </li>
 <li>
 Abhishek Das et al. "Learning Cooperative Visual Dialog Agents with Deep Reinforcement Learning." ICCV, 2017.
 </li>
 <li>
 Karl Moritz Hermann et al. "Grounded Language Learning in a Simulated 3D World." arXiv, 2017.
 </li>
 <li>
 Alexander G. Huth et al. "Natural speech reveals the semantic maps that tile human cerebral cortex." Nature, 2016.
 </li>
 <li>
 Alexander G. Huth, et al. "Decoding the semantic content of natural movies from human brain activity." Frontiers in systems neuroscience, 2016.
 </li>
 <li>
 Piotr Mirowski et al. "Learning to Navigate in Cities Without a Map." arXiv, 2018.
 </li>
 <li>
 Stevan Harnad. "The symbol grounding problem." CNLS, 1989.
 </li>
 <li>
 E Kolve, R Mottaghi, D Gordon, Y Zhu, A Gupta, A Farhadi. "AI2-THOR: An Interactive 3D Environment for Visual AI." arXiv, 2017.
 </li>
 <li>
 Yi Wu et al. "House3D: A Rich and Realistic 3D Environment." arXiv, 2017.
 </li>
 <li>
 Angel Chang et al. "Matterport3D: Learning from RGB-D Data in Indoor Environments." arXiv, 2017.
 </li>
 <li>
 Abhishek Das et al. "Embodied Question Answering." CVPR, 2018.
 </li>
 <li>
 Peter Anderson et al. "Vision-and-Language Navigation: Interpreting visually-grounded navigation instructions in real environments." CVPR, 2018.
 </li>
 <li>
 Fei Xia et al. "Gibson Env: Real-World Perception for Embodied Agents." CVPR, 2018.
 </li>
 <li>
 Manolis Savva et al. "MINOS: Multimodal indoor simulator for navigation in complex environments." arXiv, 2017.
 </li>
 <li>
 Daniel Gordon, Aniruddha Kembhavi, Mohammad Rastegari, Joseph Redmon, Dieter Fox, Ali Farhadi. "IQA: Visual Question Answering in Interactive Environments." CVPR, 2018.
 </li>
    </ol>
  </div>
</div>
