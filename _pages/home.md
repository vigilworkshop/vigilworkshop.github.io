---
layout: project
urltitle:  "Visually Grounded Interaction and Language (ViGIL)"
title: "Visually Grounded Interaction and Language (ViGIL)"
categories: nips, neurips, vancouver, canada, workshop, computer vision, natural language, grounding, interaction, machine learning, vigil, 2019, neurips19
permalink: /
bibtex: true
paper: true
acknowledgements: ""
---

<br>
<div class="row">
  <div class="col-xs-12">
    <center><h1>Visually Grounded Interaction and Language (ViGIL)</h1></center>
    <center><h2>NeurIPS 2019 Workshop, Vancouver, Canada</h2></center>
    <center><span style="color:#e74c3c;font-weight:400;">
      Friday, 13th December, 08:30 AM to 06:30 PM, Room: TBA
    </span></center>
  </div>
</div>

<div class="row" id="intro">
    <div class="col-xs-12">
        <h2>Introduction</h2>
        <p>
        The dominant paradigm in modern natural language understanding is learning statistical language models from text-only corpora. This approach is founded on a distributional notion of semantics, i.e. that the "meaning" of a word is based only on its relationship to other words. While effective for many applications, methods in this family suffer from limited semantic understanding, as they miss learning from the multimodal and interactive environment in which communication often takes place - the symbols of language thus are not grounded in anything concrete. The symbol grounding problem first highlighted this limitation, that "meaningless symbols (i.e.) words cannot be grounded in anything but other meaningless symbols" [1].
        </p>

        <p>
        On the other hand, humans acquire language by communicating about and interacting within a rich, perceptual environment. This behavior provides the necessary grounding for symbols, i.e. to concrete objects or concepts (i.e. physical or psychological). Thus, recent work has aimed to bridge vision, interactive learning, and natural language understanding through language learning tasks based on natural images (ReferIt [2], Visual Question Answering [3-6], Visual Dialog [7-10], Captioning [11, 32-35], Visual-Audio Correspondence [30]) or through embodied agents performing interactive tasks [12-29] in physically simulated environments (DeepMind Lab [12], Baidu XWorld [13], Habitat [14], StreetLearn [18], AI2-THOR [21], House3D [22], Matterport3D [23], GIBSON [27], MINOS [28]), often drawing on the recent successes of deep learning and reinforcement learning. We believe this line of research poses a promising, long-term solution to the grounding problem faced by current, popular language understanding models.
        </p>


        <p>
        While machine learning research exploring visually-grounded language learning may be in its earlier stages, it may be possible to draw insights from the rich research literature on human language acquisition. In neuroscience, recent progress in fMRI technology has enabled better understanding of the interaction between language, vision and other modalities [17,18] suggesting that the brains share neural representations of concepts across vision and language. In concurrent work, developmental cognitive scientists have argued that word acquisition in children is closely linked to them learning the underlying physical concepts in the real world [15, 31], and that they generalize surprisingly well at this from sparse evidence [36].
        </p>

        <p>
        This workshop thus aims to gather people from various backgrounds - machine learning, computer vision, natural language processing, neuroscience, cognitive science, psychology, and philosophy - to share and debate their perspectives on why grounding may (or may not) be important in building machines that truly understand natural language.
        </p>
    </div>
</div>

<br>
<div class="row" id="dates">
  <div class="col-xs-12">
    <h2>Important Dates</h2>
  </div>
</div>

<br>
<div class="row">
  <div class="col-xs-12">
    <table class="table table-striped">
      <tbody>
        <tr>
          <td>Paper Submission Deadline</td>
          <td>September 18, 2019 (11:59 PM Pacific time)</td>
        </tr>
        <tr>
          <td>Decision Notifications</td>
          <td><strike>September 26, 2019</strike> September 28, 2019</td>
        </tr>
        <tr>
          <td>Workshop</td>
          <td>December 13, 2019</td>
        </tr>
      </tbody>
    </table>
  </div>
</div>

<br>
<div class="row" id="cfp">
  <div class="col-xs-12">
    <h2>Call for Papers</h2>
  </div>
</div>

<br>
<div class="row">
    <div class="col-xs-12">
        <p>
          We invite high-quality paper submissions on the following topics:
        </p>
        <p>
            <ul>
              <li>language acquisition or learning through interactions</li>
              <li>image/video captioning, visual dialogues, visual question-answering, and other visually grounded language challenges</li>
              <li>reasoning in language and vision</li>
              <li>transfer learning in language and vision tasks</li>
              <li>audiovisual scene understanding and generation </li>
              <li>navigation and question answering in virtual worlds with natural-language instructions</li>
              <li>original multimodal works that can be extended to vision, language or interaction</li>
              <li>human-machine interaction with vision and language</li>
              <li>understanding and modeling the relationship between language and vision in humans semantic systems and modeling of natural language and visual stimuli representations in the human brain</li>
              <li>epistemology and research reflexions about language grounding, human embodiment and other related topics</li>
              <li>visual and linguistic cognition in infancy and/or adults</li>
            </ul>
        </p>
    </div>
</div>

<br>
<div class="row">
    <div class="col-xs-12">
        <h3>Submission</h3>
        <p>
            <br>
            Please upload submissions at: <a style="color:#2980b9;font-weight:400;" href="https://cmt3.research.microsoft.com/VIGIL2019/">cmt3.research.microsoft.com/VIGIL2019</a>
        </p>
        <p>
            Accepted papers will be presented during joint poster sessions, with exceptional submissions selected for spotlight oral presentations. Accepted papers will be made publicly available as non-archival reports, allowing future submissions to archival conferences or journals.
        </p>
        <p>
            Submissions should be up to 4 pages excluding references, acknowledgements, and supplementary material, and should follow <a href="https://neurips.cc/Conferences/2019/PaperInformation/StyleFiles" style="color:#2980b9;font-weight:400;">NeurIPS format</a>. The CMT-based review process will be double-blind to avoid potential conflicts of interests.
        </p>
        <p>
            We welcome published papers from *non-ML* conferences that are within the scope of the workshop (without re-formatting). These specific papers do not have to be anonymous. They are eligible for poster sessions and will only have a very light review process. 
        </p>
         <p>
            A limited pool of NeurIPS registrations might be available for accepted papers.
        </p>
        <p>
            In case of any issues, feel free to email the workshop organizers at: <a href="mailto:vigilworkshop@gmail.com">vigilworkshop@gmail.com</a>.
        </p> 
    </div>
</div>


<br>
<div class="row">
    <div class="col-xs-12">
        <h3>D-Day Workshop Information</h3>
        <p>
            <br>
            Posters will be taped to the wall (we will provide tape).
            Please make sure they are printed on lightweight paper without lamination
            and no larger than 36 x 48 inches or 90 x 122 cm in portrait orientation.
            
        </p> 
    </div>
</div>
 
<hr>
<div class="row" id="schedule">
    <div class="col-xs-12">
        <h2>Schedule</h2>
    </div>
</div>

TBA

<hr>
<div class="row" id="speakers">
  <div class="col-xs-12">
    <h2>Invited Speakers</h2>
  </div>
</div>
<p><br /></p>

<div class="row">
  <div class="col-md-12">
    <img class="speaker-pic" style="float:left;margin-right:50px;" src="/static/img/people/linda-smith.jpg" />
    <p><b>Linda Smith</b>
    is a Distinguished Professor, Psychological and Brain Sciences at Indiana University. Her recent work at the intersection of cognitive development and machine learning focuses specifically on the statistics of infants' visual experience and how it affects concept and word learning.
    <span style="color:#2980b9;">[<a href="http://www.iub.edu/~cogdev/">Webpage</a>]</span></p>
  </div>
</div>
<p><br /></p>

<div class="row">
  <div class="col-md-12">
    <img class="speaker-pic" style="float:left;margin-right:50px;" src="/static/img/people/josh-tenenbaum.jpeg" />
    <p><b>Josh Tenenbaum</b> is a Professor in Computational Cognitive Science at MIT. His work studies learning and reasoning in humans and machines, with the twin goals of understanding human intelligence in computational terms and bringing artificial intelligence closer to human-level capacities.
    <span style="color:#2980b9;">[<a href="https://web.mit.edu/cocosci/josh.html">Webpage</a>]</span></p>
  </div>
</div>
<p><br /></p>

<div class="row">
  <div class="col-md-12">
    <img class="speaker-pic" style="float:left;margin-right:75px;" src="/static/img/people/jay-mcclelland.jpg" />
    <p><b>Jay McClelland</b> is a Professor in the Psychology Department and Director of the Center for Mind, Brain and Computation at Stanford University. His research spans a broad range of topics in cognitive science and cognitive neuroscience, including perception and perceptual decision making; learning and memory; language and reading; semantic and mathematical cognition; and cognitive development.
    <span style="color:#2980b9;">[<a href="https://stanford.edu/~jlmcc/">Webpage</a>]</span></p>
  </div>
</div>
<p><br /></p>

<div class="row">
  <div class="col-md-12">
    <img class="speaker-pic" style="float:left;margin-right:50px;" src="/static/img/people/jesse-thomason.jpg" />
    <p><b>Jesse Thomason</b> is a postdoctoral researcher at the University of Washington. His research focuses on language grounding and natural language processing applications for robotics, including how dialog with humans can facilitate both robot task execution and learning.
    <span style="color:#2980b9;">[<a href="https://jessethomason.com">Webpage</a>]</span></p>
  </div>
</div>
<p><br /></p>

<div class="row">
  <div class="col-md-12">
    <img class="speaker-pic" style="float:left;margin-right:50px;" src="/static/img/people/lisa-anne-hendricks.jpg" />
    <p><b>Lisa Anne Hendricks</b> is a research scientist at DeepMind (previously a PhD student in Computer Vision at UC Berkeley). Her work focuses on building systems which can express information about visual content using natural language and retrieve visual information given natural language.
    <span style="color:#2980b9;">[<a href="https://people.eecs.berkeley.edu/~lisa_anne/">Webpage</a>]</span></p>
  </div>
</div>
<p><br /></p>

<div class="row">
  <div class="col-md-12">
    <img class="speaker-pic" style="float:left;margin-right:80px;" src="/static/img/people/tim-lillicrap.png" />
    <p><b>Timothy Lillicrap</b> is a research scientist at DeepMind. His research focuses on machine learning and statistics for optimal control and decision making, as well as using these mathematical frameworks to understand how the brain learns. He has developed algorithms and approaches for exploiting deep neural networks in the context of reinforcement learning, and recurrent memory architectures for one-shot learning.
    <span style="color:#2980b9;">[<a href="http://contrastiveconvergence.net/~timothylillicrap/index.php">Webpage</a>]</span></p>
  </div>
</div>
<p><br /></p>

<div class="row">
  <div class="col-md-12">
    <img class="speaker-pic" style="float:left;margin-right:80px;" src="/static/img/people/jason-baldridge.png" />
    <p><b>Jason Baldridge</b> is a research scientist at Google. His research focuses on the theoretical and applied aspects of computational linguistics
      -- from formal and computational models of syntax to machine learning for natural language processing and geotemporal grounding of natural language.
    <span style="color:#2980b9;">[<a href="http://www.jasonbaldridge.com">Webpage</a>]</span></p>
  </div>
</div>
<p><br /></p>

<div class="row">
  <div class="col-md-12">
    <img class="speaker-pic" style="float:left;margin-right:55px;" src="/static/img/people/louis-philippe-morency.jpg" />
    <p><b>Louis-Philippe Morency</b> is an Associate Professor at the Language Technology Institute at Carnegie Mellon University. His research focuses on building the computational foundations to enable computers with the abilities to analyze, recognize and predict subtle human communicative behaviors during social interactions.
    <span style="color:#2980b9;">[<a href="https://www.cs.cmu.edu/~morency/">Webpage</a>]</span></p>
  </div>
</div>
<p><br /></p>

<hr>
<div class="row" id="speakers">
  <div class="col-xs-12">
    <h2>Accepted Papers</h2>
  </div>
</div>
<p><br /></p>

<ul class="paper-list">
    <li>
        <span class="paper-title">Help, Anna! Visual Navigation with Natural Multimodal Assistance via Retrospective Curiosity-Encouraging Imitation Learning</span><br>
        <span class="paper-authors">Khanh Nguyen (University of Maryland)*; Hal Daume (University of Maryland, College Park)</span><br>
        <span class="paper-meta">[<a href="static/papers/1.pdf">PDF</a>]</span>
    </li>
    <li>
        <span class="paper-title">What is needed for simple spatial language capabilities in VQA?</span><br>
        <span class="paper-authors">Alexander Kuhnle (University of Cambridge)*; Ann Copestake (University of Cambridge)</span><br>
        <span class="paper-meta">[<a href="static/papers/2.pdf">PDF</a>] [<a href="static/papers/2_supp.pdf">Supplementary</a>]</span>
    </li>
    <li>
        <span class="paper-title">Learning from Observation-Only Demonstration for Task-Oriented Language Grounding via Self-Examination</span><br>
        <span class="paper-authors">Tsu-Jui Fu (UC Santa Barbara); Yuta Tsuboi (Preferred Networks)*; Sosuke Kobayashi (Preferred Networks); Yuta Kikuchi (Preferred Networks)</span><br>
        <span class="paper-meta">[<a href="static/papers/3.pdf">PDF</a>]</span>
    </li>
    <li>
        <span class="paper-title">Not All Actions Are Equal: Learning to Stop in Language-Grounded Urban Navigation</span><br>
        <span class="paper-authors">Jiannan Xiang (University of Science and Technology of China); Xin Wang (University of California, Santa Barbara)*; William Yang Wang (UC Santa Barbara)</span><br>
        <span class="paper-meta">[<a href="static/papers/5.pdf">PDF</a>]</span>
    </li>
    <li>
        <span class="paper-title">Hidden State Guidance: Improving Image Captioning Using an Image Conditioned Autoencoder</span><br>
        <span class="paper-authors">Jialin Wu (UT Austin)*; Raymond Mooney (Univ. of Texas at Austin)</span><br>
        <span class="paper-meta">[<a href="static/papers/6.pdf">PDF</a>]</span>
    </li>
    <li>
        <span class="paper-title">Situated Grounding Facilitates Multimodal Concept Learning for AI</span><br>
        <span class="paper-authors">Nikhil Krishnaswamy (Brandeis University)*; James Pustejovsky (Brandeis University)</span><br>
        <span class="paper-meta">[<a href="static/papers/7.pdf">PDF</a>]</span>
    </li>
    <li>
        <span class="paper-title">VideoNavQA: Bridging the Gap between Visual and Embodied Question Answering</span><br>
        <span class="paper-authors">Cătălina Cangea (University of Cambridge)*; Eugene Belilovsky (Mila); Pietro Liò (University of Cambridge); Aaron Courville (Universite de Montreal)</span><br>
        <span class="paper-meta">[<a href="static/papers/9.pdf">PDF</a>]</span>
    </li>
    <li>
        <span class="paper-title">Induced Attention Invariance: Defending VQA Models against Adversarial Attacks</span><br>
        <span class="paper-authors">Vasu Sharma (Carnegie Mellon University)*; Ankita Kalra (CMU); Louise-Phillipe Morency (Carnegie Mellon University)</span><br>
        <span class="paper-meta">[<a href="static/papers/11.pdf">PDF</a>]</span>
    </li>
    <li>
        <span class="paper-title">Natural Language Grounded Multitask Navigation</span><br>
        <span class="paper-authors">Xin Wang (University of California, Santa Barbara)*; Vihan Jain (Google Research); Eugene Ie (Google Research); William Yang Wang (UC Santa Barbara); Zornitsa Kozareva (Google Cloud); Sujith Ravi (Google Research)</span><br>
        <span class="paper-meta">[<a href="static/papers/13.pdf">PDF</a>]</span>
    </li>
    <li>
        <span class="paper-title">Contextual Grounding of Natural Language Entities in Images</span><br>
        <span class="paper-authors">Farley Lai (NEC Laboratories America, Inc.)*; Ning Xie (Wright State University); Derek Doran (Wright State University); Asim Kadav (NEC Labs)</span><br>
        <span class="paper-meta">[<a href="static/papers/14.pdf">PDF</a>]</span>
    </li>
    <li>
        <span class="paper-title">Visual Dialog for Radiology: Data Curation and FirstSteps</span><br>
        <span class="paper-authors">Olga Kovaleva (UMass Lowell)*; Chaitanya  Shivade (IBM Research); Satyananda Kashyap (IBM Research); Karina Kanjaria (IBM Research); Adam Coy (IBM Research); Deddeh Ballah (IBM Research); Yufan Guo (IBM Research); Joy Wu (IBM Research); Alexandros Karargyris (IBM Research); David Beymer (IBM); Anna Rumshisky (University of Massachusetts Lowell); Vandana Mukherjee (IBM Research)</span><br>
        <span class="paper-meta">[<a href="static/papers/15.pdf">PDF</a>]</span>
    </li>
    <li>
        <span class="paper-title">Multimodal Generative Learning Utilizing Jensen-Shannon-Divergence</span><br>
        <span class="paper-authors">Thomas Sutter ()*; Imant Daunhawer (ETH Zurich); Julia Vogt (ETH Zurich)</span><br>
        <span class="paper-meta">[<a href="static/papers/16.pdf">PDF</a>]</span>
    </li>
    <li>
        <span class="paper-title">Learning Question-Guided Video Representation for Multi-Turn Video Question Answering</span><br>
        <span class="paper-authors">Guan-Lin Chao (Carnegie Mellon University)*; Abhinav Rastogi (Google); Semih Yavuz (University of California, Santa Barbara); Dilek Hakkani-Tur (Amazon Alexa AI); Jindong Chen (Google); Ian Lane (Carnegie Mellon University)</span><br>
        <span class="paper-meta">[<a href="static/papers/24.pdf">PDF</a>]</span>
    </li>
    <li>
        <span class="paper-title">Structural and functional learning for learning language use</span><br>
        <span class="paper-authors">Angeliki Lazaridou (DeepMind)*; Anna Potapenko (DeepMind); Olivier Tieleman (DeepMind)</span><br>
        <span class="paper-meta">[<a href="static/papers/25.pdf">PDF</a>]</span>
    </li>
    <li>
        <span class="paper-title">Community size effect in artificial learning systems</span><br>
        <span class="paper-authors">Olivier Tieleman (DeepMind)*; Angeliki Lazaridou (DeepMind); Shibl Mourad (DeepMind); Charles  Blundell  (DeepMind); Doina Precup (DeepMind)</span><br>
        <span class="paper-meta">[<a href="static/papers/26.pdf">PDF</a>]</span>
    </li>
    <li>
        <span class="paper-title">A Comprehensive Analysis of Semantic Compositionality in Text-to-Image Generation</span><br>
        <span class="paper-authors">Chihiro Fujiyama (Ochanomizu University)*; Ichiro  kobayashi (ochanomizu university tokyo)</span><br>
        <span class="paper-meta">[<a href="static/papers/29.pdf">PDF</a>]</span>
    </li>
    <li>
        <span class="paper-title">Recurrent Instance Segmentation using Sequences of Referring Expressions</span><br>
        <span class="paper-authors">Alba Maria Hererra-Palacio (Universitat Politecnica de Catalunya); Carles Ventura (Universitat Oberta de Catalunya); Carina Silberer (Universitat Pompeu Fabra); Ionut-Teodor Sorodoc (Universitat Pompeu Fabra); Gemma Boleda (Universitat Pompeu Fabra); Xavier Giro-i-Nieto (Universitat Politecnica de Catalunya)*</span><br>
        <span class="paper-meta">[<a href="static/papers/30.pdf">PDF</a>] [<a href="static/papers/30_supp.pdf">Supplementary</a>]</span>
    </li>
    <li>
        <span class="paper-title">Modulated Self-attention Convolutional Network for VQA</span><br>
        <span class="paper-authors">Jean-Benoit Delbrouck (UMONS)*</span><br>
        <span class="paper-meta">[<a href="static/papers/32.pdf">PDF</a>]</span>
    </li>
    <li>
        <span class="paper-title">General Evaluation for Instruction Conditioned Navigation using Dynamic Time Warping</span><br>
        <span class="paper-authors">Gabriel Ilharco (University of Washington)*; Vihan Jain (Google Research); Alexander Ku (Google Research); Eugene Ie (Google Research); Jason Baldridge (Google Inc.)</span><br>
        <span class="paper-meta">[<a href="static/papers/33.pdf">PDF</a>]</span>
    </li>
    <li>
        <span class="paper-title">A Simple Baseline for Visual Commonsense Reasoning</span><br>
        <span class="paper-authors">Jingxiang Lin (UIUC)*; Unnat Jain (UIUC); Alexander Schwing (UIUC)</span><br>
        <span class="paper-meta">[<a href="static/papers/34.pdf">PDF</a>]</span>
    </li>
    <li>
        <span class="paper-title">Language Grounding through Social Interactions and Curiosity-Driven Multi-Goal Learning</span><br>
        <span class="paper-authors">Nicolas Lair (Inserm U1093 CAPS)*; Cédric Colas (Inria Bordeaux - Sud-Ouest); Rémy Portelas (Inria Bordeaux - Sud-Ouest); Jean-Michel Dussoux (Cloud Temple); Peter  Dominey (INSERM); Pierre-Yves Oudeyer (Inria)</span><br>
        <span class="paper-meta">[<a href="static/papers/35.pdf">PDF</a>]</span>
    </li>
    <li>
        <span class="paper-title">Deep compositional robotic planners that follow natural language commands</span><br>
        <span class="paper-authors">Yen-Ling Kuo (MIT)*; Boris Katz (MIT); Andrei Barbu (MIT)</span><br>
        <span class="paper-meta">[<a href="static/papers/37.pdf">PDF</a>]</span>
    </li>
    <li>
        <span class="paper-title">Can adversarial training learn image captioning ?</span><br>
        <span class="paper-authors">Jean-Benoit Delbrouck (UMONS)*</span><br>
        <span class="paper-meta">[<a href="static/papers/38.pdf">PDF</a>]</span>
    </li>
    <li>
        <span class="paper-title">Leveraging Topics and Audio Features with Multimodal Attention for Audio Visual Scene-Aware Dialog</span><br>
        <span class="paper-authors">Shachi H Kumar (Intel Labs); Eda Okur (Intel Labs)*; Saurav Sahay (Intel); Jonathan Huang (Intel); Lama Nachman (Intel Labs)</span><br>
        <span class="paper-meta">[<a href="static/papers/39.pdf">PDF</a>]</span>
    </li>
    <li>
        <span class="paper-title">Shaping Visual Representations with Language for Few-shot Classification</span><br>
        <span class="paper-authors">Jesse Mu (Stanford University)*; Percy Liang (Stanford University); Noah Goodman (Stanford University)</span><br>
        <span class="paper-meta">[<a href="static/papers/41.pdf">PDF</a>]</span>
    </li>
    <li>
        <span class="paper-title">Self-Educated Language Agent with Hindsight Experience Replay for Instruction Following</span><br>
        <span class="paper-authors">Geoffrey Cideron (University of Lille)*; Mathieu Seurin (University of Lille); Florian Strub (DeepMind); Olivier Pietquin (Google Research - Brain Team)</span><br>
        <span class="paper-meta">[<a href="static/papers/42.pdf">PDF</a>]</span>
    </li>
    <li>
        <span class="paper-title">Analyzing Compositionality in Visual Question Answering</span><br>
        <span class="paper-authors">Sanjay Subramanian (Allen Institute for Artificial Intelligence)*; Sameer  Singh (University of California, Irvine); Matt Gardner (AI2)</span><br>
        <span class="paper-meta">[<a href="static/papers/43.pdf">PDF</a>]</span>
    </li>
    <li>
        <span class="paper-title">A perspective on multi-agent communication for information fusion</span><br>
        <span class="paper-authors">Homagni Saha (Iowa state university)*; Vijay Venkataraman (Honeywell); Alberto Speranzon (Honeywell); Soumik Sarkar (Iowa State University)</span><br>
        <span class="paper-meta">[<a href="static/papers/46.pdf">PDF</a>]</span>
    </li>
    <li>
        <span class="paper-title">Cross-Modal Mapping for Generalized Zero-Shot Learning by Soft-Labeling</span><br>
        <span class="paper-authors">Shabnam Daghaghi ()*; Anshumali Shrivastava (Rice University); Tharun Medini (Rice University)</span><br>
        <span class="paper-meta">[<a href="static/papers/47.pdf">PDF</a>]</span>
    </li>
    <li>
        <span class="paper-title">Learning Language from Vision</span><br>
        <span class="paper-authors">Candace Ross (Massachusetts Institute of Technology); Cheahuychou Mao (MIT); Boris Katz (MIT); Andrei Barbu (MIT)*</span><br>
        <span class="paper-meta">[<a href="static/papers/48.pdf">PDF</a>]</span>
    </li>
    <li>
        <span class="paper-title">Commonsense and Semantic-Guided Navigation through Language in Embodied Environment</span><br>
        <span class="paper-authors">Dian Yu (University of California, Davis)*; Chandra Khatri (Uber); Alexandros Papangelis (UberAI); Andrea Madotto (Hong Kong University of Science and Technology); Mahdi Namazifar (Uber Technologies, Inc.); Joost Huizinga (UberAI); Adrien Ecoffet (UberAI); Huaixiu Zheng (Uber Technologies); Piero Molino (Uber AI); Jeff Clune (Uber AI Labs); Zhou Yu (UC Davis); Kenji Sagae (University of California, Davis); Gokhan Tur (Uber)</span><br>
        <span class="paper-meta">[<a href="static/papers/49.pdf">PDF</a>]</span>
    </li>
</ul>

<hr>
<div class="row" id="organizers">
  <div class="col-xs-12">
    <h2>Organizers</h2>
  </div>
</div>

<br>
<div class="row">
  <div class="col-xs-6 col-lg-3">
    <a href="https://fstrub95.github.io/">
      <img class="people-pic" src="{{ "/static/img/people/florianstrub.png" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://fstrub95.github.io/">Florian Strub</a>
      <h6>University of Lille, Inria | DeepMind</h6>
    </div>
  </div>
  <div class="col-xs-6 col-lg-3">
    <a href="http://www-etud.iro.umontreal.ca/~devries/">
      <img class="people-pic" src="{{ "/static/img/people/harmdevries.jpg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="http://www-etud.iro.umontreal.ca/~devries/">Harm de Vries</a>
      <h6>University of Montreal</h6>
    </div>
  </div>
  <div class="col-xs-6 col-lg-3">
    <a href="http://wijmans.xyz/">
      <img class="people-pic" src="{{ "/static/img/people/erikwijmans.jpg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="http://wijmans.xyz/">Erik Wijmans</a>
      <h6>Georgia Tech</h6>
    </div>
  </div>
  <div class="col-xs-6 col-lg-3">
    <a href="https://www.linkedin.com/in/drew-a-hudson">
      <img class="people-pic" src="{{ "/static/img/people/drew.jpg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://www.linkedin.com/in/drew-a-hudson">Drew Hudson</a>
      <h6>Stanford University</h6>
    </div>
  </div>
  <div class="col-xs-6 col-lg-3">
    <a href="http://alanesuhr.com/">
      <img class="people-pic" src="{{ "/static/img/people/alane.png" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="http://alanesuhr.com/">Alane Suhr</a>
      <h6>Cornell University</h6>
    </div>
  </div>
  <div class="col-xs-6 col-lg-3">
    <a href="https://abhishekdas.com">
      <img class="people-pic" src="{{ "/static/img/people/abhishek-das-dp.jpg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://abhishekdas.com">Abhishek Das</a>
      <h6>Georgia Tech</h6>
    </div>
  </div>
  <div class="col-xs-6 col-lg-3">
    <a href="https://www.cc.gatech.edu/~slee3191/">
      <img class="people-pic" src="{{ "/static/img/people/stefan.png" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://www.cc.gatech.edu/~slee3191/">Stefan Lee</a>
      <h6>Oregon State University</h6>
    </div>
  </div>
</div>

<hr>
<div class="row" id="scientific_committee">
  <div class="col-xs-12">
    <h2>Scientific Committee</h2>
  </div>
</div>

<br>
<div class="row">
  <div class="col-xs-6 col-lg-3">
    <a href="http://www.mateuszmalinowski.com/">
      <img class="people-pic" src="{{ "/static/img/people/mateusz-malinowski-dp.jpeg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="http://www.mateuszmalinowski.com/">Mateusz Malinowski</a>
      <h6>DeepMind</h6>
    </div>
  </div>
  <div class="col-xs-6 col-lg-3">
    <a href="https://nlp.stanford.edu/manning/">
      <img class="people-pic" src="{{ "/static/img/people/chris-manning.jpg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://nlp.stanford.edu/manning/">Chris Manning</a>
      <h6>Stanford University</h6>
    </div>
  </div>
  <div class="col-xs-6 col-lg-3">
    <a href="https://mila.quebec/en/person/aaron-courville/">
      <img class="people-pic" src="{{ "/static/img/people/aaron-courville-dp.jpg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://mila.quebec/en/person/aaron-courville/">Aaron Courville</a>
      <h6>University of Montreal</h6>
    </div>
  </div>
  <div class="col-xs-6 col-lg-3">
    <a href="http://www.lifl.fr/~pietquin/">
      <img class="people-pic" src="{{ "/static/img/people/olivier-pietquin-dp.jpg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="http://www.lifl.fr/~pietquin/">Olivier Pietquin</a>
      <h6>Google Brain</h6>
    </div>
  </div>
</div>

<hr>
<div class="row">
  <div class="col-xs-12">
    <h2>Program Committee</h2>
  </div>
</div>
<br>
<div class="row">
  <div class="col-xs-6">
    <ul>
      <li>Aishwarya Agrawal</li>
      <li>Cătălina Cangea</li>
      <li>Volkan Cirik</li>
      <li>Meera Hahn</li>
      <li>Ethan Perez</li>
      <li>Rowan Zellers</li>
    </ul>
  </div>
  <div class="col-xs-6">
    <ul>
      <li>Ryan Benmalek</li>
      <li>Luca Celotti</li>
      <li>Daniel Fried</li>
      <li>Arjun Majumdar</li>
      <li>Hao Tan</li>
    </ul>
  </div>
</div>

<hr>
<div class="row">
  <div class="col-xs-12">
    <h2>Previous sessions</h2>
  </div>
</div>
<a name="/prev_session"></a>
<div class="row">
    <div class="col-xs-12">
        <p>
            <ul>
                <li><a href="https://nips2017vigil.github.io/">ViGIL Workshop at NeurIPS 2017</a></li>
                <li><a href="https://nips2018vigil.github.io/">ViGIL Workshop at NeurIPS 2018</a></li>
            </ul>
        </p>
    </div>
</div>

<div class="row">
  <div class="col-xs-12">
    <h2>Sponsors</h2>
  </div>
</div>
<a name="/sponsors"></a>
<div class="row">
  <div class="col-xs-4">
    <img src="{{ "/static/img/ico/ivado.jpg" | prepend:site.baseurl }}">
  </div>
  <div class="col-xs-4">
    <img style="margin-top:10px;" src="{{ "/static/img/ico/deepmind_logo.png" | prepend:site.baseurl }}">
  </div>
  <div class="col-xs-4">
    <img style="margin-top:10px;" src="{{ "/static/img/ico/GoogleAI.png" | prepend:site.baseurl }}">
  </div>
</div>
<br>

<hr>
<div class="row">
  <div class="col-xs-12">
    <h2>References</h2>
  </div>
</div>
<div class="row">
  <div class="col-md-12">
    <ol>
    <li>
      <a href="https://www.sciencedirect.com/science/article/pii/0167278990900876" target="_blank">
        Stevan Harnad. "The symbol grounding problem." CNLS, 1989.
      </a>
    </li>
    <li>
      <a target="_blank" href="http://tamaraberg.com/papers/referit.pdf">
        Sahar Kazemzadeh et al. "ReferItGame: Referring to Objects in Photographs of Natural Scenes." EMNLP, 2014.
      </a>
    </li>
    <li>
      <a href="https://arxiv.org/abs/1505.00468" target="_blank">
        Stanislaw Antol et al. "VQA: Visual question answering." ICCV, 2015.
      </a>
    </li>
    <li>
      <a href="https://arxiv.org/abs/1505.01121" target="_blank">
        Mateusz Malinowski et al. "Ask Your Neurons: A Neural-based Approach to Answering Questions about Images." ICCV, 2015.
      </a>
    </li>
    <li>
      <a href="https://arxiv.org/abs/1410.0210" target="_blank">
        Mateusz Malinowski et al. "A Multi-World Approach to Question Answering about Real-World Scenes based on Uncertain Input." NIPS, 2014.
      </a>
    </li>
    <li>
      <a href="https://www.pnas.org/content/112/12/3618" target="_blank">
        Geman Donald, et al. "Visual Turing test for computer vision systems." PNAS, 2015.
      </a>
    </li>
    <li>
      <a href="https://arxiv.org/abs/1611.08669" target="_blank">
        Abhishek Das et al. "Visual dialog." CVPR, 2017.
      </a>
    </li>
    <li>
      <a href="https://arxiv.org/abs/1611.08481" target="_blank">
        Harm de Vries et al. "GuessWhat?! Visual object discovery through multi-modal dialogue." CVPR, 2017.
      </a>
    </li>
    <li>
      <a href="https://arxiv.org/abs/1703.06585" target="_blank">
        Abhishek Das et al. "Learning Cooperative Visual Dialog Agents with Deep Reinforcement Learning." ICCV, 2017.
      </a>
    </li>
    <li>
      <a href="https://arxiv.org/abs/1901.09107" target="_blank">
        Huda Alamri et al. "Audio-Visual Scene-Aware Dialog" CVPR, 2019.
      </a>
    </li>
    <li>
      <a href="https://arxiv.org/abs/1704.01518" target="_blank">
        Anna Rohrbach et al. "Generating Descriptions with Grounded and Co-Referenced People." CVPR, 2017.
      </a>
    </li>
    <li>
      <a href="https://arxiv.org/abs/1612.03801" target="_blank">
        Charles Beattie et al. Deepmind Lab. arXiv, 2016.
      </a>
    </li>
    <li>
      <a href="https://arxiv.org/abs/1805.08329" target="_blank">
        Haonan Yu et al. "Guided Feature Transformation (GFT): A Neural Language Grounding Module for Embodied Agents." arXiv, 2018.
      </a>
    </li>
    <li>
      <a href="https://arxiv.org/abs/1904.01201" target="_blank">
        Habitat: A Platform for Embodied AI Research. 2019.
      </a>
    </li>
    <li>
      <a href="https://psycnet.apa.org/record/1985-09186-001" target="_blank">
        Alison Gopnik et al. "Semantic and cognitive development in 15- to 21-month-old children." Journal of Child Language, 1984.
      </a>
    </li>    
    <li>
      <a href="https://arxiv.org/abs/1706.06551" target="_blank">
        Karl Moritz Hermann et al. "Grounded Language Learning in a Simulated 3D World." arXiv, 2017.
      </a>
    </li>
    <li>
      <a href="https://www.nature.com/articles/nature17637" target="_blank">
        Alexander G. Huth et al. "Natural speech reveals the semantic maps that tile human cerebral cortex." Nature, 2016.
      </a>
    </li>
    <li>
      <a href="https://www.ncbi.nlm.nih.gov/pubmed/27781035" target="_blank">
        Alexander G. Huth et al. "Decoding the semantic content of natural movies from human brain activity." Frontiers in systems neuroscience, 2016.
      </a>
    </li>
    <li>
      <a href="https://arxiv.org/abs/1804.00168" target="_blank">
        Piotr Mirowski et al. "Learning to Navigate in Cities Without a Map." NeurIPS, 2018.
      </a>
    </li>
    <li>
      <a href="https://arxiv.org/abs/1903.00401" target="_blank">
        Karl Moritz Hermann et al. "Learning to Follow Directions in StreetView." arXiv, 2019. 
      </a>
    </li>
    <li>
      <a href="https://arxiv.org/abs/1712.05474" target="_blank">
        E Kolve et al. "AI2-THOR: An Interactive 3D Environment for Visual AI." arXiv, 2017.
      </a>
    </li>
    <li>
      <a href="https://arxiv.org/abs/1801.02209" target="_blank">
        Yi Wu et al. "House3D: A Rich and Realistic 3D Environment." arXiv, 2017.
      </a>
    </li>
    <li>
      <a href="https://arxiv.org/abs/1709.06158" target="_blank">
        Angel Chang et al. "Matterport3D: Learning from RGB-D Data in Indoor Environments." arXiv, 2017.
      </a>
    </li>
    <li>
      <a href="https://arxiv.org/abs/1711.11543" target="_blank">
        Abhishek Das et al. "Embodied Question Answering." CVPR, 2018.
      </a>
    </li>
    <li>
      <a href="https://arxiv.org/abs/1711.07280" target="_blank">
        Peter Anderson et al. "Vision-and-Language Navigation: Interpreting visually-grounded navigation instructions in real environments." CVPR, 2018.
      </a>
    </li>
    <li>
      <a href="https://arxiv.org/abs/1811.10092" target="_blank">
        Xin Wang et al. "Reinforced Cross-Modal Matching and Self-Supervised Imitation Learning for Vision-Language Navigation." CVPR, 2019.
      </a>
    </li>
    <li>
      <a href="https://arxiv.org/abs/1808.10654" target="_blank">
        Fei Xia et al. "Gibson Env: Real-World Perception for Embodied Agents." CVPR, 2018.
      </a>
    </li>
    <li>
      <a href="https://arxiv.org/abs/1712.03931" target="_blank">
        Manolis Savva et al. "MINOS: Multimodal indoor simulator for navigation in complex environments." arXiv, 2017.
      </a>
    </li>
    <li>
      <a href="https://arxiv.org/abs/1712.03316" target="_blank">
        Daniel Gordon et al. "IQA: Visual Question Answering in Interactive Environments." CVPR, 2018.
      </a>
    </li>
    <li>
      <a href="https://arxiv.org/abs/1705.08168" target="_blank">
        Relja Arandjelovic et al. "Look, Listen and Learn." ICCV, 2017.
      </a>
    </li>
    <li>
      <a href="Quantity and Diversity: Simulating Early Word Learning
Environments" target="_blank">Jessica Montag et al. "Quantity and Diversity: Simulating Early Word Learning Environments." Cognitive Science, 2018.</a>
    </li>
    <li>
      <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Vinyals_Show_and_Tell_2015_CVPR_paper.html" target="_blank">
        Oriol Vinyals et al. "Show and Tell: A Neural Image Caption Generator." CVPR, 2015.
      </a>
    </li>
    <li>
      <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Karpathy_Deep_Visual-Semantic_Alignments_2015_CVPR_paper.html" target="_blank">Andrej Karpathy et al. "Deep Visual-Semantic Alignments for Generating Image Descriptions." CVPR, 2015.</a>
    </li>
    <li>
      <a href="http://openaccess.thecvf.com/content_cvpr_2015/html/Donahue_Long-Term_Recurrent_Convolutional_2015_CVPR_paper.html" target="_blank">Jeff Donahue et al. "Long-Term Recurrent Convolutional Networks for Visual Recognition and Description." CVPR, 2015.</a>
    </li>
    <li>
      <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/Hendricks_Deep_Compositional_Captioning_CVPR_2016_paper.html" target="_blank">Lisa Anne Hendricks et al. "Deep Compositional Captioning: Describing Novel Object Categories Without Paired Training Data." CVPR, 2016.</a>
    </li>
    <li>
      <a href="https://science.sciencemag.org/content/350/6266/1332">Brendan Lake et al. "Human-level concept learning through probabilistic program induction." Science, 2015.</a>
    </li>
    </ol>
  </div>
</div>
